---
title: "Plugin framework"
description: "Deliver events to external systems with TCP, HTTP, log, or process plugins backed by a durable queue."
icon: "shuffle"
---

Every committed event lands in a RocksDB-backed queue that feeds plugin instances. Plugins choose which slices to consume—event payloads, materialized state snapshots, schema definitions, or combinations—so specialized services can build optimized read models without touching the write path.

## Installation and configuration

```bash
dbx plugin install <plugin> <version> --source <path|url> [--bin <file>] [--checksum <sha256>] [--force]
```

Process plugins unpack into `~/.eventdbx/plugins/<plugin>/<version>/<target>/`, where `<target>` matches the host triple (for example, `x86_64-apple-darwin`). Bind them to a named instance and wire arguments/env vars as needed:

```bash
dbx plugin config process \
  --name search \
  --plugin dbx_search \
  --version 1.0.0 \
  --arg "--listen=0.0.0.0:8081" \
  --env SEARCH_CLUSTER=https://example.com

dbx plugin enable search
```

Other built-in emitters:

- `dbx plugin config tcp --name ingest --host host --port 9000 --payload event-only`
- `dbx plugin config http --name webhook --endpoint https://hooks.acme.dev --header Authorization=Bearer..`
- `dbx plugin config log --name audit --level info --template "aggregate={aggregate} event={event}"`

Disable, enable, or remove plugins with `dbx plugin disable <name>`, `dbx plugin enable <name>`, and `dbx plugin remove <name>`.

## Job queue management

```bash
dbx queue          # inspect pending/dead jobs
dbx queue retry    # retry failed deliveries immediately
dbx queue clear    # purge dead entries (prompts for confirmation)
```

Use `dbx plugin test` to run connectivity checks before enabling, and `dbx plugin list` to see every configured instance. Failed deliveries back off exponentially until they succeed or the aggregate disappears.

## Replay and targeting

```bash
dbx plugin replay <plugin-name> <aggregate> [<aggregate_id>]
```

Replays resend stored events for a single aggregate type or id through the selected plugin, which is useful after deploying a new downstream service. Combine with domain scoping via `dbx checkout -d <domain>` to keep replays isolated per bounded context.

## Example payload

Plugins receive an `EventRecord` JSON envelope over TCP/HTTP/log emitters or Cap'n Proto streams for process plugins:

```json
{
  "aggregate_type": "patient",
  "aggregate_id": "p-001",
  "event_type": "patient-updated",
  "payload": { "status": "inactive", "comment": "Archived via API" },
  "extensions": { "@analytics": { "correlation_id": "rest-1234" } },
  "metadata": {
    "event_id": "1234567890123",
    "created_at": "2024-12-01T17:22:43.512345Z",
    "issued_by": { "group": "admin", "user": "jane" }
  },
  "version": 5,
  "hash": "cafe…",
  "merkle_root": "deadbeef…"
}
```

> **Heads-up for plugin authors**  
> The [`dbx_plugins`](https://github.com/thachp/dbx_plugins) surfaces now receive `event_id` values as Snowflake strings and may optionally see an `extensions` object. Treat `metadata.event_id` as a stringified Snowflake and ignore or consume `extensions` as needed.

## Automation with watchers

```bash
dbx watch remote1 --mode push --interval 120 --background
dbx watch remote1 --mode bidirectional --aggregate ledger --run-once
dbx watch remote1 --skip-if-active
dbx watch status remote1
```

`watch` daemons trigger push, pull, or bidirectional cycles at the requested interval, optionally daemonizing with `--background`. `--skip-if-active` prevents overlap when another watcher is already working on the same domain, and `dbx watch status --all` prints a summary of every watcher.
